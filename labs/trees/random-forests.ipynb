{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.datasets import load_boston, load_iris, load_wine, load_digits, \\\n",
    "                             load_breast_cancer, load_diabetes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from rfpimp import *\n",
    "\n",
    "from sklearn import tree\n",
    "from dtreeviz.trees import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rent(n=None, bootstrap=False):\n",
    "    df_rent = pd.read_csv(\"data/rent-ideal.csv\")\n",
    "    if n is None:\n",
    "        n = len(df_rent)\n",
    "    df_rent = df_rent.sample(n, replace=bootstrap)\n",
    "    X = df_rent[['bedrooms','bathrooms','latitude','longitude']]\n",
    "    y = df_rent['price']\n",
    "    return X, y\n",
    "\n",
    "def boston():\n",
    "    boston = load_boston()\n",
    "    X = boston.data\n",
    "    y = boston.target\n",
    "    features = boston.feature_names\n",
    "    df = pd.DataFrame(data=X,columns=features)\n",
    "    df['y'] = y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "Get the `rent-ideal.csv`  data file from canvas and store in the data directory underneath your notebook  directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = rent()\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train random forests of different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to train a random forest  that has a single tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=1)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the MAE for the training and the testing set, printing them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train = mean_absolute_error(...)\n",
    "mae = mean_absolute_error(...)\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and testing cycle several times to see the variance: the test scores bounce around a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the number of trees (`n_estimators`) to 2, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=2)\n",
    "rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice the both test MAE scores going down and bouncing around less from run to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  Why does the MAE score go down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    With 2 trees, the chances are that the random forest will have seen (trained on) more of the original training set, despite bootstrapping.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the number of trees (`n_estimators`) to 10, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=10)\n",
    "rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  What you notice about the MAE scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "They are getting smaller.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  After running several times, what else do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    With 10 trees, the prediction from run to run varies a lot less. We have reduced variance,  improving generality.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the number of trees (`n_estimators`) to 200, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=200)\n",
    "%time rf.fit(X_train, y_train) # how long does this take?\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  What you notice about the MAE scores from a single run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "They are a bit smaller, but not by much.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it took a long time to train, about 10 seconds.  Do the exact same thing again but this time use `n_jobs=-1` as an argument to the `RandomForestRegressor` constructor. This tells the library to use all processing cores available on the computer processor. As long as the data is not too huge (because it must pass it around), it often goes much faster using this argument. It should take less than two seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  What you notice about the MAE scores from SEVERAL runs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The variance is even lower (tighter).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altering bootstrap size\n",
    "\n",
    "Jeremy Howard has an awesome trick that he uses during development of a model to convince sklearn random forests to use less than $n$ observations from the training data to train each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jeremy_trick_RF_sample_size(n):\n",
    "    from sklearn.ensemble import forest\n",
    "    forest._generate_sample_indices = \\\n",
    "        (lambda rs, n_samples: forest.check_random_state(rs).randint(0, n_samples,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 38,000 training records, change that to 19,000 and check the accuracy again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeremy_trick_RF_sample_size(round(len(X_train)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a less accurate, but it's faster. (Down to 1.27s from 1.86s on my machine.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**  Why is it less accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Each tree is seeing less of the data set during training.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn off bootstrapping by adding `bootstrap=False` to the constructor of the model. This means that it will subsample rather than bootstrap. Remember that bootstrapping gets about two thirds of the data because of replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1, bootstrap=False)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That brings the accuracy backup a little bit for the test set but very much so for the training MAE score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop that size to one third of the training records then retrain and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeremy_trick_RF_sample_size(round(len(X_train)/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine is twice as fast as the full bootstrap but continues to have very tight variance because of the number of trees. The accuracy is lower, however, about what we get for the  usual random forest with two trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we forget, let's reset the bootstrapping size back to the usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jeremy_trick_reset_RF_sample_size():\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeremy_trick_reset_RF_sample_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF prediction confidence\n",
    "\n",
    "A random forest is a collection of decision trees, each of which contributes a prediction. The forest averages those predictions to provide the overall prediction. Let's dig inside the random forest to get the individual trees out and ask them what their predictions are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a random forest with 10 trees on `X_train`, `y_train`.  Use `for t in rf.estimators_` to iterate through the trees making predictions with `t` not `rf`. Print out the usual MAE scores for each tree predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "for t in ...:\n",
    "    mae_train = ...\n",
    "    mae = ...\n",
    "    print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "for t in rf.estimators_:\n",
    "    mae_train = mean_absolute_error(y_train, t.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, t.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it bounces around quite a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one of the `X_test` rows and print out the addicted rent price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ... # pick single test case\n",
    "x = x.values.reshape(1,-1) # Needs to be a one-row matrix\n",
    "\n",
    "print(f\"{x} => {rf.predict(x)}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "x = X_test.iloc[3,:] # pick single test case\n",
    "x = x.values.reshape(1,-1)\n",
    "print(f\"{x} => {rf.predict(x)}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the forest came to that conclusion. Compute the average of the predictions obtained from every tree.  Compare that to the prediction obtained directly from the random forest (`rf.predict(X_test)`). They should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ...\n",
    "print(f\"{x} => {y_pred}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "y_pred = np.mean([t.predict(x) for t in rf.estimators_])\n",
    "print(f\"{x} => {y_pred}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the standard deviation of the tree estimates and print that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "np.std([t.predict(x) for t in rf.estimators_])\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the standard deviation, the more tightly grouped the predictions were, which means we should have more confidence in our answer. \n",
    "\n",
    "Different records will often have different standard deviations, which means we could have different levels of confidence in the various answers. This might be helpful to a bank for example that wanted to not only predict whether to give loans, but how confident the model was."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
