{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with imbalanced data sets (credicard.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtreeviz.trees import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, f1_score, accuracy_score,\\\n",
    "                            roc_auc_score, average_precision_score, precision_recall_curve, auc,\\\n",
    "                            roc_curve\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_boston, load_iris, load_wine, load_digits, \\\n",
    "                             load_breast_cancer, load_diabetes, fetch_mldata\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from copy import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/mlg-ulb/creditcardfraud/downloads/creditcardfraud.zip/3\n",
    "df = pd.read_csv(\"../../data/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num anomalies 492/284807 = 0.17%\n"
     ]
    }
   ],
   "source": [
    "print(f\"num anomalies {np.sum(df['Class']==1)}/{len(df)} = {100*np.sum(df['Class']==1)/len(df):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_estimators=10, n_jobs=-1, class_weight=None) #'balanced'\n",
    "model = LogisticRegression(solver='lbfgs', class_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try usual train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('Class', axis=1), df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "clf = copy(model)#RandomForestClassifier(n_estimators=10, n_jobs=-1, class_weight=None) #'balanced'\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1] # 2nd column is p(fraud)\n",
    "AUC = roc_auc_score(y_test, y_pred_proba)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba, pos_label=1)\n",
    "PR = auc(recall, precision)\n",
    "FPR, TPR, _ = roc_curve(y_test, y_pred_proba)\n",
    "df_conf = pd.DataFrame(confusion, columns=['F','T'], index=['F','T'])\n",
    "df_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 {f1_score(y_test, y_pred):.2f}, Accuracy {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"ROC {AUC:.2f}, Avg PR curve {PR:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'png'\n",
    "plt.plot(FPR, TPR, 'b.-', markersize=1, alpha=0.5)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(f\"credit card ROC curve\")\n",
    "plt.savefig(\"/tmp/ROC-curve.png\", dpi=200)\n",
    "plt.show()\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'png'\n",
    "plt.plot(recall, precision, 'b.-', markersize=1, alpha=0.5)\n",
    "plt.xlabel(\"recall\")\n",
    "plt.ylabel(\"precision\")\n",
    "plt.title(f\"credit card PR curve\")\n",
    "plt.savefig(\"/tmp/PR-curve.png\", dpi=200)\n",
    "plt.show()\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out proper test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must split out test set first but get 20% from each class at original ratio of fraud/good\n",
    "df_good = df[df['Class']==0]\n",
    "df_fraud = df[df['Class']==1]\n",
    "\n",
    "df_train_good, df_test_good   = train_test_split(df_good, test_size=0.20)\n",
    "df_train_fraud, df_test_fraud = train_test_split(df_fraud, test_size=0.20)\n",
    "\n",
    "df_train = pd.concat([df_train_good, df_train_fraud], axis=0)\n",
    "df_test  = pd.concat([df_test_good, df_test_fraud], axis=0)\n",
    "\n",
    "print(f\"TRAIN num fraud {np.sum(df_train['Class']==1)}/{len(df_train)} = {100*np.sum(df_train['Class']==1)/len(df_train):.2f}%\")\n",
    "print(f\"TEST num fraud {np.sum(df_test['Class']==1)}/{len(df_test)} = {100*np.sum(df_test['Class']==1)/len(df_test):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train.drop('Class', axis=1), df_train['Class']\n",
    "X_test, y_test = df_test.drop('Class', axis=1), df_test['Class']\n",
    "clf = copy(model)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "print(f\"F1 {f1_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Accuracy {accuracy_score(y_test, y_pred):.2f}\")\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1] # 2nd column is p(fraud)\n",
    "AUC = roc_auc_score(y_test, y_pred_proba)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba, pos_label=1)\n",
    "PR = auc(recall, precision)\n",
    "print(f\"ROC {AUC:.2f}, Avg PR curve {PR:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversample fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, upsample the fraud records in training set\n",
    "df_good = df_train[df_train['Class']==0]\n",
    "df_fraud = df_train[df_train['Class']==1]\n",
    "\n",
    "df_fraud_balanced = df_fraud.sample(int(len(df_fraud)*10), replace=True)\n",
    "df_train_upsampled = pd.concat([df_good, df_fraud_balanced], axis=0)\n",
    "print(f\"Upsampled TRAIN num fraud {np.sum(df_train_upsampled['Class']==1)}/{len(df_train_upsampled)} = {100*np.sum(df_train_upsampled['Class']==1)/len(df_train_upsampled):.2f}%\")\n",
    "print(f\"TEST num fraud {np.sum(df_test['Class']==1)}/{len(df_test)} = {100*np.sum(df_test['Class']==1)/len(df_test):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train_upsampled.drop('Class', axis=1), df_train_upsampled['Class']\n",
    "X_test, y_test = df_test.drop('Class', axis=1), df_test['Class']\n",
    "clf = copy(model)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "print(f\"F1 {f1_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Accuracy {accuracy_score(y_test, y_pred):.2f}\")\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1] # 2nd column is p(fraud)\n",
    "AUC = roc_auc_score(y_test, y_pred_proba)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba, pos_label=1)\n",
    "PR = auc(recall, precision)\n",
    "print(f\"ROC {AUC:.2f}, Avg PR curve {PR:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
