{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie reviews sentiment analysis with Naive Bayes\n",
    "\n",
    "Project 2, MS621<br>\n",
    "*Fall 2019*\n",
    "\n",
    "## Goal\n",
    "\n",
    "In this project, you will build a multinomial naive bayes classifier to predict whether a movie review is positive or negative.  As part of the project, you will also learn to do *k*-fold cross validation testing.\n",
    "\n",
    "You will do your work in a `bayes`-*userid* repository. Please keep all of your files in the root directory of the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Download and uncompress [polarity data set v2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz) into the root directory of your repository, but do not add the data to git.  My directory looks like:\n",
    "\n",
    "```\n",
    "$ ls\n",
    "bayes.py   review_polarity  test_bayes.py\n",
    "```\n",
    "\n",
    "Download the [test_bayes.py](https://github.com/parrt/msds621/blob/master/projects/bayes/test_bayes.py) script into the root directory of your repository; you can add this if you want but I will overwrite it when testing. It assumes that the `review_polarity` directory is in the same directory (the root of the repository).\n",
    "\n",
    "Download the [bayes.py starter kit](https://github.com/parrt/msds621/blob/master/projects/bayes/bayes.py) into the root directory of your repository. Make sure to add this to the repo.\n",
    "\n",
    "See Naive Bayes discussion, p258 in [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/).\n",
    "\n",
    "**Please do not add the data to your repository!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive bayes classifiers for text documents\n",
    "\n",
    "A text classifier predicts to which class, $c$, an unknown document $d$ belongs. In our case, the predictions are binary: $c=0$ for negative movie review and $c=1$ for positive movie review. We can think about classification mathematically as picking the most likely class:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~P(c|d)\n",
    "$$\n",
    "\n",
    "We can replace $P(c|d)$, using Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(c | d) = \\frac{P(c)P(d|c)}{P(d)}\n",
    "$$\n",
    "\n",
    "to getg the formula \n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~\\frac{P(c)P(d|c)}{P(d)}\n",
    "$$\n",
    "\n",
    "Since $P(d)$ is a constant for any given document $d$, we can use the following equivalent but simpler formula:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~ P(c)P(d|c)\n",
    "$$\n",
    "\n",
    "Training then consists of estimating $P(c)$ and $P(c|d)$, which will get to shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing documents\n",
    "\n",
    "Text classification requires a representation for document $d$. When loading a document, we first load the text and then tokenize the words, stripping away punctuation and stop words like *the*. The list of words is a fine representation for a document except that each document has a different length, which makes training most models problematic as they assume tabular data with a fixed number of features.  The simplest and most common approach is to create an overall vocabulary, $V$, created as a set of unique words across all documents in all classes. Then, the training features are those words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to represent a document than is with a binary word vector, with a 1 in each column if that word is present in the document. Something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>food</th>\n",
       "      <th>hong</th>\n",
       "      <th>kong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat  food  hong  kong\n",
       "0    1     1     0     0\n",
       "1    0     0     1     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=[[1,1,0,0],\n",
    "                        [0,0,1,1]], columns=['cat','food','hong','kong'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tends to work well for very short strings/documents, such as article titles or tweets. For longer documents, using a binary presence or absence loses information. Instead, it's better to count the number of times each word is present.  For example, here are 3 documents and resulting word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>food</th>\n",
       "      <th>hong</th>\n",
       "      <th>kong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat  food  hong  kong\n",
       "0    3     1     0     0\n",
       "1    0     0     2     2\n",
       "2    1     0     1     1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = \"cats food cats cats\"\n",
    "d2 = \"hong kong hong kong\"\n",
    "d3 = \"cats in hong kong\"  # assume we strip stop words like \"in\"\n",
    "df = pd.DataFrame(data=[[3,1,0,0],\n",
    "                        [0,0,2,2],\n",
    "                        [1,0,1,1]],\n",
    "                  columns=['cat','food','hong','kong'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These word vectors with fixed lengths are how most models expect data, including sklearn's implementation. (It's assuming Gaussian distributions for probability estimates where as we are assuming multinomial, but we can still shove our data in.)  Here's how to train a Naive Bayes model with sklearn using the trivial/toy `df` data and get the training set error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct = 3 / 3 = 100.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "X = df.values\n",
    "y = [0, 1, 1] # assume document classes\n",
    "sknb = GaussianNB()\n",
    "sknb.fit(X, y)\n",
    "y_pred = sknb.predict(X)\n",
    "print(f\"Correct = {np.sum(y==y_pred)} / {len(y)} = {100*np.sum(y==y_pred) / len(y):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is convenient to keep word vectors in a 2D matrix and it is what sklearn likes, we will use the same representation in this project. Given the directory name, your function `load_docs()` will return a list of word lists where each word list is the raw list of tokens, typically with repeated words. Then,  function `vocab()` will create the combined vocabulary as a mapping from word to  word feature index, starting with index 1.  Index 0 is reserved for unknown words.  Vocabulary $V$ should be a `defaultdict(int)` so that unknown words get mapped to value/index 0. Finally, function `vectorize()` will convert that to a 2D matrix, one row per document:\n",
    "\n",
    "```\n",
    "neg = load_docs(neg_dir)\n",
    "pos = load_docs(pos_dir)\n",
    "V = vocab(neg,pos)\n",
    "vneg = vectorize_docs(neg, V)\n",
    "vpos = vectorize_docs(pos, V)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we need to estimate $P(c)$ and $P(d|c)$ for all classes and documents. Estimating $P(c)$ is easy: it's just the number of documents in class $c$ divided by the total number of documents. To estimate $P(d | c)$, Naive Bayes assumes that each word is *conditionally independent*, given the class, meaning:\n",
    "\n",
    "$$\n",
    "P(d | c) = \\prod_{w \\in d} P(w | c)\n",
    "$$\n",
    "\n",
    "so that gives us:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~ P(c) \\prod_{w \\in d} P(w | c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $w$ is not a unique word in $d$, so the product includes $P(w|c)$ 5 times if $w$ appears 5 times in $d$.\n",
    "\n",
    "Because we are going to use word counts, not binary word vectors, in fixed-length vectors, we need to include $P(w|c)$ explicitly multiple times for repeated $w$ in $d$:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~ P(c) \\prod_{unique(w) \\in d} P(w | c)^{n_w(d)}\n",
    "$$\n",
    "\n",
    "where $n_w(d)$ is the number of times $w$ occurs in $d$.\n",
    "\n",
    "Now we have to figure out how to estimate $P(w | c)$, the probability of seeing word $w$ given that we're looking at a document from class $c$. That's just the number of times $w$ appears in all documents from class $c$ divided by the total number of words (including repeats) in all documents from class $c$:\n",
    "\n",
    "$$P(w | c) = \\frac{count(w,c)}{count(c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "\n",
    "Once we have the appropriate parameter estimates, we have a model that can make predictions in an ideal setting:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} ~ P(c) \\prod_{unique(w) \\in d} P(w | c)^{n_w(d)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Floating point underflow\n",
    "\n",
    "The first problem involves the limitations of floating-point arithmetic in computers. Because the probabilities are less than one and there could be tens of thousands multiplied together, we risk floating-point underflow. That just means that eventually the product will attenuate to zero and our classifier is useless.  The solution is simply to take the log of the right hand side because it is monotonic and won't affect the $argmax$:\n",
    "\n",
    "$$\n",
    "c^*= \\underset{c}{argmax} \\left \\{ log(P(c)) + \\sum_{unique(w) \\in d} log(P(w | c)^{n_w(d)}) \\right \\}\n",
    "$$\n",
    "\n",
    "Or,\n",
    "\n",
    "$$\n",
    "c^* = \\underset{c}{argmax} \\left \\{ log(P(c)) + \\sum_{unique(w) \\in d} n_w(d) \\times log(P(w | c)) \\right \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding log(0)\n",
    "\n",
    "If word $w$ does not exist in class $c$ (but is in $V$), then the classifier will try to evaluate $log(0)$, which gets an error.  To solve the problem, we use *Laplace Smoothing*, which just means adding 1 to each word count in $n_w(d)$ when computing $P(w|c)$ and making sure to compensate by adding $|V|$ to the denominator (adding 1 for each vocabulary word):\n",
    "\n",
    "$$P(w | c) = \\frac{count(w,c) + 1}{count(c) + |V|}$$\n",
    "\n",
    "where $|V|$ is the size of the vocabulary for all documents in all classes.  Adding this to the denominator, keeps the $P(w|c)$ ratio the same. This way, even if $count(w,c)$ is 0, this ratio > 0.  (Note: Each doc's word vector is size $|V|$. During training, any $w$ not found in docs of $c$, will have word count 0. Summing these gets us just total number of words in $c$. However, when we add +1, then $c$ looks like it has every word in $V$.  Hence, we must divide by $|V|$ not $|V_c|$.)\n",
    "\n",
    "In your project, we can deal with both the Laplace smoothing by adding one to the data frame or 2-D matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>food</th>\n",
       "      <th>hong</th>\n",
       "      <th>kong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat  food  hong  kong\n",
       "0    4     2     1     1\n",
       "1    1     1     3     3\n",
       "2    2     1     2     2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = df+1; df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then computing the class word counts from this incremented matrix.  The last two rows are from category 1, the positive reviews so we can isolate those and compute word counts per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>food</th>\n",
       "      <th>hong</th>\n",
       "      <th>kong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat  food  hong  kong\n",
       "1    1     1     3     3\n",
       "2    2     1     2     2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpos = df_.iloc[1:3,:]; vpos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word counts for documents in the positive category are then found using matrix operation `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat     3\n",
       "food    2\n",
       "hong    5\n",
       "kong    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_word_counts = vpos.sum(axis=0); pos_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $P(w|c)$ for all $w$ is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in pos docs: 15\n",
      "P(w|c):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cat     0.200000\n",
       "food    0.133333\n",
       "hong    0.333333\n",
       "kong    0.333333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_total_words = np.sum(pos_word_counts)\n",
    "print(f\"Total words in pos docs: {pos_total_words}\")\n",
    "print(f\"P(w|c):\")\n",
    "pos_word_counts / pos_total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important that you get used to using these vector operations for productivity and performance reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dealing with missing words\n",
    "\n",
    "Laplace smoothing deals with $w$ that are in the vocabulary $V$ but that are not in a class, hence, giving $count(w,c)=0$ for some $c$. There's one last slightly different problem. If a future unknown document contains a word not in $V$ (i.e., not in the training data), then what should $count(w,c)$ be?  Probably not 0 because that would mean we had data indicating it does not appear in class $c$ when we have *no* training data on it.\n",
    "\n",
    "To be strictly correct and keep $P(w | c)$ a probability in the presence of unknown words, all we have to do is add 1 to the denominator in addition to the Laplace smoothing changes:\n",
    "\n",
    "$$P(w | c) = \\frac{count(w,c) + 1}{count(c) + |V| + 1}$$\n",
    "\n",
    "We are lumping all unknown words into a single \"wildcard\" word that exists in every $V$. That has the effect of increasing the overall vocabulary size for class $c$ to include room for an unknown word (and all unknown words map to that same spot). In this way, an unknown word gets probability:\n",
    "\n",
    "$$P(unknown|c) = \\frac{0 + 1}{count(c) + |V| + 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, this is no big deal as all classes will get the same nudge for the unknown word so classification won't be affected.\n",
    "\n",
    "To deal with unknown words in the project, we can reserve word index 0 to mean unknown word. All words in the training vocabulary start at index 1. So, if we normally have $|V|$ words in the training vocabulary, we will now have $|V|+1$ and each class will also have $|V|+1$ words since no word has 0 word count. Each word vector will be of length $|V|+1$.  \n",
    "\n",
    "When we count the words per class with `vneg.sum(axis=0)` and `vpos.sum(axis=0)`, these sums will include the \"+1\" we added for Laplace smoothing.  Since we have augmented the vocabulary for unknown words at index 0, this will also increase `word_count_per_class` values since we added \"+1\" to word index 0 as well with `df+1`. So the formula for estimating $P(w|c)$ remains:\n",
    "\n",
    "```\n",
    "pos_word_counts / pos_total_words\n",
    "```\n",
    "\n",
    "To make a prediction for an unknown document, $d$, you will be given a feature vector composed of the word counts from $d$. Sum the multiplication of the word counts for $w \\in d$ by the log of $P(w|c)$ for class $c$ to get the weighted sum, then add the log of the class likelihood $P(c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed issues\n",
    "\n",
    "For large data sets, Python loops often are too slow and so we have to rely on vector operations, which are implemented in C. For example, the `predict(X)` method receives a 2D matrix of word vectors and must make a prediction for each one. The temptation is to write the very readable:\n",
    "\n",
    "```\n",
    "y_pred = []\n",
    "for d in X:\n",
    "    y_pred = prediction for d\n",
    "return y_pred\n",
    "```\n",
    "\n",
    "But, you should use the built-in `numpy` functions such as `np.dot` (same as the `@` operator) and apply functions across vectors. For example, if I have a vector, $v$, and I'd like the log of each value, don't write a loop. Use `np.log(v)`, which will give you a vector with the results.\n",
    "\n",
    "My `predict()` method consists primarily of a matrix-vector multiplication per class  followed by `argmax`. My implementation is twice as fast as sklearn and appears to be more accurate for 4-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "To submit your project, ensure that your `bayes.py` file is submitted to your repository. That file must be in the root of your `bayes`-*userid* repository.  It should not have a main program and should consist of a collection of functions. You must implement the following functions:\n",
    "\n",
    "* `load_docs(docs_dirname)`\n",
    "* `vocab(neg, pos)`\n",
    "* `vectorize_docs(docs, V)`\n",
    "* `kfold_CV(model, X, y, k=4)`\n",
    "\n",
    "and implement class `NaiveBayes621` with these methods\n",
    "\n",
    "* `__init__(self)` (The body of this function is just keyword `pass`)\n",
    "* `fit(self, X, y)`\n",
    "* `predict(self, X)`\n",
    "\n",
    "\n",
    "**Please do not add the data to your repository!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "In your github `bayes`-*userid* repository, you should submit your `bayes.py` file in the root directory. It should not have a main program that runs when the file is imported.\n",
    "\n",
    "*Please do not add data files to your repository!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To evaluate your projects I will run `test_bayes.py` from your repo root directory. Here is a sample test run:\n",
    "\n",
    "```\n",
    "$ python -m pytest -v test_bayes.py \n",
    "============================================== test session starts ============================\n",
    "platform darwin -- Python 3.7.1, pytest-4.0.2, py-1.7.0, pluggy-0.8.0 -- ...\n",
    "cachedir: .pytest_cache\n",
    "rootdir: /Users/parrt/courses/msds621-private/projects/bayes, inifile:\n",
    "plugins: remotedata-0.3.1, openfiles-0.3.1, doctestplus-0.2.0, arraydiff-0.3\n",
    "collected 6 items                                                                                                \n",
    "\n",
    "test_bayes.py::test_load PASSED                                                                            [ 16%]\n",
    "test_bayes.py::test_vocab PASSED                                                                           [ 33%]\n",
    "test_bayes.py::test_vectorize PASSED                                                                       [ 50%]\n",
    "test_bayes.py::test_training_error PASSED                                                                  [ 66%]\n",
    "test_bayes.py::test_kfold_621 PASSED                                                                       [ 83%]\n",
    "test_bayes.py::test_kfold_sklearn_vs_621 PASSED                                                            [100%]\n",
    "\n",
    "=========================================== 6 passed in 21.04 seconds ============================\n",
    "```\n",
    "\n",
    "Notice that it takes about 20 seconds. If your project takes more than one minute, I will take off 10 points from 100. Each test is evaluated in a binary fashion: it either works or it does not. Each failed test cost you 15 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
